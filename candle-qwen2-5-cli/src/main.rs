use anyhow::Result;
use clap::{Parser, ValueEnum};
use std::io::Write;
use tracing_chrome::ChromeLayerBuilder;
use tracing_subscriber::prelude::*;

use candle_qwen2_5_core::{ModelArgs, Qwen2Model, Which as CoreWhich};

const DEFAULT_PROMPT: &str = "Write a Rust function to calculate the factorial of a given number.";

#[derive(Clone, Debug, Copy, PartialEq, Eq, ValueEnum)]
enum Which {
    #[value(name = "0.5b")]
    W25_0_5b,
    #[value(name = "1.5b")]
    W25_1_5b,
    #[value(name = "3b")]
    W25_3b,
    #[value(name = "7b")]
    W25_7b,
}

#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
struct Args {
    /// GGUF file to load, typically a .gguf file generated by the quantize command from llama.cpp
    #[arg(long)]
    model: Option<String>,

    /// The initial prompt.
    #[arg(long)]
    prompt: Option<String>,

    /// The length of the sample to generate (in tokens).
    #[arg(short = 'n', long, default_value_t = 1000)]
    sample_len: usize,

    /// The tokenizer config in json format.
    #[arg(long)]
    tokenizer: Option<String>,

    /// The temperature used to generate samples, use 0 for greedy sampling.
    #[arg(long, default_value_t = 0.8)]
    temperature: f64,

    /// Nucleus sampling probability cutoff.
    #[arg(long)]
    top_p: Option<f64>,

    /// Only sample among the top K samples.
    #[arg(long)]
    top_k: Option<usize>,

    /// The seed to use when generating random samples.
    #[arg(long, default_value_t = 299792458)]
    seed: u64,

    /// Enable tracing (generates a trace-timestamp.json file).
    #[arg(long)]
    tracing: bool,

    /// Process prompt elements separately.
    #[arg(long)]
    split_prompt: bool,

    /// Run on CPU rather than GPU even if a GPU is available.
    #[arg(long)]
    cpu: bool,

    /// Penalty to be applied for repeating tokens, 1. means no penalty.
    #[arg(long, default_value_t = 1.1)]
    repeat_penalty: f32,

    /// The context size to consider for the repeat penalty.
    #[arg(long, default_value_t = 64)]
    repeat_last_n: usize,

    /// The model size to use.
    #[arg(long, default_value = "0.5b")]
    which: Which,
}

impl From<Which> for CoreWhich {
    fn from(w: Which) -> Self {
        match w {
            Which::W25_0_5b => CoreWhich::W25_0_5b,
            Which::W25_1_5b => CoreWhich::W25_1_5b,
            Which::W25_3b => CoreWhich::W25_3b,
            Which::W25_7b => CoreWhich::W25_7b,
        }
    }
}

fn main() -> Result<()> {
    let args = Args::parse();
    let _guard = if args.tracing {
        let (chrome_layer, guard) = ChromeLayerBuilder::new().build();
        tracing_subscriber::registry().with(chrome_layer).init();
        Some(guard)
    } else {
        None
    };

    let model_args = ModelArgs {
        model: args.model,
        sample_len: args.sample_len,
        tokenizer: args.tokenizer,
        temperature: args.temperature,
        top_p: args.top_p,
        top_k: args.top_k,
        seed: args.seed,
        tracing: args.tracing,
        split_prompt: args.split_prompt,
        cpu: args.cpu,
        repeat_penalty: args.repeat_penalty,
        repeat_last_n: args.repeat_last_n,
        which: args.which.into(),
    };

    let mut model = Qwen2Model::new(&model_args)?;

    let prompt_str = args.prompt.unwrap_or_else(|| DEFAULT_PROMPT.to_string());

    let stats = model.generate(&prompt_str, model_args.sample_len, |token| {
        print!("{token}");
        std::io::stdout().flush()?;
        Ok(())
    })?;

    println!(
        "\n\n{:4} prompt tokens processed: {:.2} token/s",
        stats.prompt_tokens,
        stats.prompt_tokens as f64 / stats.prompt_processing_time.as_secs_f64(),
    );
    println!(
        "{:4} tokens generated: {:.2} token/s",
        stats.generated_tokens,
        stats.generated_tokens as f64 / stats.generation_time.as_secs_f64(),
    );

    Ok(())
}
